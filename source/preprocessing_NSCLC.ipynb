{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for NSCLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download and Organize NSCLC Datasets\n",
    "\n",
    "Download the following public NSCLC datasets from The Cancer Imaging Archive (TCIA):\n",
    "\n",
    "- [NSCLC-Radiomics](https://www.cancerimagingarchive.net/collection/nsclc-radiomics/)\n",
    "- [NSCLC-Radiomics-Interobserver1](https://www.cancerimagingarchive.net/collection/nsclc-radiomics-interobserver1/)\n",
    "- [RIDER-LungCT-Seg](https://www.cancerimagingarchive.net/collection/rider-lung-ct/)\n",
    "- [NSCLC Radiogenomics](https://www.cancerimagingarchive.net/collection/nsclc-radiogenomics/)\n",
    "- [LUNG-PET-CT-Dx](https://www.cancerimagingarchive.net/collection/lung-pet-ct-dx/)\n",
    "\n",
    "After downloading, organize the data into the following directory structure:\n",
    "\n",
    "```text\n",
    "/workspace/data/Original_dataset/NSCLC/\n",
    "├── NSCLC-Radiomics/\n",
    "│   ├── LUNG1-001/\n",
    "│   │   ├── ct.nii.gz           # CT image\n",
    "│   │   ├── gtv_1.nii.gz        # Tumor 1 mask (binary: 1 = tumor, 0 = background)\n",
    "│   │   ├── gtv_2.nii.gz        # Tumor 2 mask (if multiple tumors)\n",
    "│   └── ...\n",
    "├── NSCLC-Radiomics-Interobserver1/\n",
    "│   ├── interobs01/\n",
    "│   │   ├── ct.nii.gz\n",
    "│   │   ├── gtv_1.nii.gz\n",
    "│   └── ...\n",
    "├── RIDER-LungCT-Seg/\n",
    "│   ├── RIDER-0000000000/\n",
    "│   │   ├── ct.nii.gz\n",
    "│   │   ├── gtv_1.nii.gz\n",
    "│   └── ...\n",
    "├── NSCLC-Radiogenomics/\n",
    "│   ├── R01-001/\n",
    "│   │   ├── ct.nii.gz\n",
    "│   │   ├── gtv_1.nii.gz\n",
    "│   └── ...\n",
    "├── Lung-PET-CT-Dx/\n",
    "│   ├── Lung_Dx-A0001/\n",
    "│   │   ├── ct.nii.gz\n",
    "│   │   ├── gtv_1.nii.gz\n",
    "│   └── ...\n",
    "```\n",
    "\n",
    "## Tumor Subtype Labels\n",
    "In addition to organizing the image and mask files, prepare a JSON file named `gtv_labels.json` in each dataset folder (if applicable) to store the tumor subtype for each tumor **instance**.\n",
    "\n",
    "Each key should follow the format `<patient_id>_<tumor_index>`, where `tumor_index` matches the suffix in the corresponding `gtv_i.nii.gz` file.\n",
    "\n",
    "\n",
    "### Example:\n",
    "If a folder contains:\n",
    "```text\n",
    "LUNG1-001/\n",
    "├── ct.nii.gz\n",
    "├── gtv_1.nii.gz\n",
    "├── gtv_2.nii.gz\n",
    "```\n",
    "Then the `gtv_labels.json` file should look like:\n",
    "```json\n",
    "{\n",
    "  \"LUNG1-001_1\": \"SCC\",\n",
    "  \"LUNG1-001_2\": \"ADC\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Allowed label values:\n",
    "- `SCC`: Squamous Cell Carcinoma\n",
    "- `ADC`: Adenocarcinoma\n",
    "- `LCC`: Large Cell Carcinoma\n",
    "- `NOS`: Not Otherwise Specified\n",
    "- `NaN`: Unknown or unavailable\n",
    "\n",
    "This label file will be used later for subtype-aware training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Resample and Pad Volumes\n",
    "\n",
    "To ensure consistency across all samples, we perform the following preprocessing steps:\n",
    "\n",
    "### 1. Resample to Isotropic Spacing\n",
    "- All CT images and tumor masks are resampled to a uniform voxel spacing of **(1.5, 1.5, 1.5)** mm using:\n",
    "  - **Linear interpolation** for CT images\n",
    "  - **Nearest-neighbor interpolation** for binary tumor masks\n",
    "\n",
    "### 2. Zero-Padding for Minimum Size\n",
    "- The image encoder requires input volumes of shape **(128, 128, 128)**.\n",
    "- If a resampled volume is smaller along any axis, **zero-padding** is applied:\n",
    "  - Padding is added to the **end** of each axis (i.e., inferior, posterior, or right side).\n",
    "  - For CT images, the padding value is set to the **minimum intensity** of the image.\n",
    "  - For masks, the padding value is set to **0** (background).\n",
    "\n",
    "### Output Structure\n",
    "Preprocessed volumes are saved in the following directory structure:\n",
    "```text\n",
    "/workspace/data/NSCLC/\n",
    "├── images/        # Resampled and padded CT images\n",
    "├── labels/        # Resampled and padded tumor masks\n",
    "```\n",
    "\n",
    "Each output file follows the naming convention:\n",
    "```\n",
    "<patient_id>_<tumor_index>.nii.gz\n",
    "```\n",
    "**Examples:**\n",
    "- `LUNG1-001_1.nii.gz`\n",
    "- `R01-001_2.nii.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mask_volume_zero(mask_sitk):\n",
    "    mask_arr = sitk.GetArrayFromImage(mask_sitk)\n",
    "    return np.sum(mask_arr) == 0\n",
    "\n",
    "def resample_volume(volume, interpolator = sitk.sitkLinear, new_spacing = [1.5, 1.5, 1.5]):\n",
    "    original_spacing = volume.GetSpacing()\n",
    "    original_size = volume.GetSize()\n",
    "    new_size = [int(round(osz*ospc/nspc)) for osz,ospc,nspc in zip(original_size, original_spacing, new_spacing)]\n",
    "\n",
    "    min_value = float(sitk.GetArrayViewFromImage(volume).min())\n",
    "    return sitk.Resample(volume, new_size, sitk.Transform(), interpolator,\n",
    "                         volume.GetOrigin(), new_spacing, volume.GetDirection(), min_value,\n",
    "                         volume.GetPixelID())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/Original_dataset/NSCLC'\n",
    "\n",
    "save_root = '/workspace/data/NSCLC'\n",
    "images_save_root = os.path.join(save_root, 'images')\n",
    "labels_save_root = os.path.join(save_root, 'labels')\n",
    "os.makedirs(images_save_root, exist_ok=True)\n",
    "os.makedirs(labels_save_root, exist_ok=True)\n",
    "\n",
    "total_gtv_labels = dict()\n",
    "gtv_types = [\"SCC\", \"ADC\", \"LCC\", \"NOS\", \"NaN\"]\n",
    "\n",
    "datasets = os.listdir(data_root)\n",
    "for dataset in datasets:\n",
    "    isGTV = True\n",
    "    gtv_label_path = os.path.join(data_root, dataset, 'gtv_labels.json')\n",
    "    if os.path.exists(gtv_label_path):\n",
    "        with open(gtv_label_path, 'r') as f:\n",
    "            gtv_labels= json.load(f)\n",
    "    else:\n",
    "        print(f'No GTV labels found for dataset {dataset}, skipping...')\n",
    "        isGTV = False\n",
    "\n",
    "    dataset_root = os.path.join(data_root, dataset)\n",
    "    print(f'Processing dataset: {dataset}')\n",
    "    pats = [f for f in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, f))]\n",
    "    for pat in tqdm(pats):\n",
    "        img_path = os.path.join(dataset_root, pat, 'ct.nii.gz')\n",
    "        img_sitk = sitk.ReadImage(img_path)\n",
    "\n",
    "        gtv_files = [f for f in os.listdir(os.path.join(dataset_root, pat)) if 'gtv' in f and f.endswith('.nii.gz')]\n",
    "        if not gtv_files:\n",
    "            print(f'{dataset}-{pat} does not have GTV mask')\n",
    "            continue\n",
    "        for gtv_file in gtv_files:\n",
    "            target_label = int(gtv_file.split('.')[0].split('_')[-1])\n",
    "\n",
    "            gtv_name = pat+'_'+str(target_label)\n",
    "            if not isGTV or gtv_name not in gtv_labels:\n",
    "                print(f'{dataset}-{pat} does not have GTV label {gtv_name}')\n",
    "                total_gtv_labels[gtv_name] = \"NaN\"\n",
    "            else:\n",
    "                if gtv_labels[gtv_name] not in gtv_types:\n",
    "                    print(f'{dataset}-{pat} has unknown GTV type {gtv_labels[gtv_name]} for {gtv_name}, setting to NaN')\n",
    "                    total_gtv_labels[gtv_name] = \"NaN\"\n",
    "                else:\n",
    "                    total_gtv_labels[gtv_name] = gtv_labels[gtv_name]\n",
    "\n",
    "            target_path = os.path.join(dataset_root, pat, gtv_file)\n",
    "            target_sitk = sitk.ReadImage(target_path)\n",
    "\n",
    "            if img_sitk.GetSize() != target_sitk.GetSize():\n",
    "                print(f'{dataset}-{pat} does not have same shape between image and target mask {img_sitk.GetSize()} != {target_sitk.GetSize()}')\n",
    "                continue\n",
    "\n",
    "            if is_mask_volume_zero(target_sitk):\n",
    "                print(f'{dataset}-{pat}-{target_label} mask is empty')\n",
    "                continue\n",
    "\n",
    "            # Resample spacing to 1.5x1.5x1.5\n",
    "            target_spacing = (1.5, 1.5, 1.5)\n",
    "            img_sitk = resample_volume(img_sitk, interpolator=sitk.sitkLinear, new_spacing=target_spacing)\n",
    "            target_sitk = resample_volume(target_sitk, interpolator=sitk.sitkNearestNeighbor, new_spacing=target_spacing)\n",
    "\n",
    "            img_arr = sitk.GetArrayFromImage(img_sitk)\n",
    "            target_arr = sitk.GetArrayFromImage(target_sitk).astype(np.uint8)\n",
    "            \n",
    "            min_size = 128\n",
    "            if img_arr.shape[0] < min_size:\n",
    "                padding_x = min_size - img_arr.shape[0]\n",
    "            elif img_arr.shape[0] % 4 != 0: # Pad to make size divisible by 4 for APE\n",
    "                padding_x = 4 - (img_arr.shape[0] % 4)\n",
    "            else:\n",
    "                padding_x = 0\n",
    "\n",
    "            if img_arr.shape[1] < min_size:\n",
    "                padding_y = min_size - img_arr.shape[1]\n",
    "            elif img_arr.shape[1] % 4 != 0: # Pad to make size divisible by 4 for APE\n",
    "                padding_y = 4 - (img_arr.shape[1] % 4)\n",
    "            else:\n",
    "                padding_y = 0\n",
    "\n",
    "            if img_arr.shape[2] < min_size:\n",
    "                padding_z = min_size - img_arr.shape[2]\n",
    "            elif img_arr.shape[2] % 4 != 0: # Pad to make size divisible by 4 for APE\n",
    "                padding_z = 4 - (img_arr.shape[2] % 4)\n",
    "            else:\n",
    "                padding_z = 0\n",
    "\n",
    "            img_arr = np.pad(img_arr, ((0, padding_x), (0, padding_y), (0, padding_z)), 'constant', constant_values=img_arr.min())\n",
    "            target_arr = np.pad(target_arr, ((0, padding_x), (0, padding_y), (0, padding_z)), 'constant', constant_values=(0,0))\n",
    "\n",
    "            img_sitk = sitk.GetImageFromArray(img_arr)\n",
    "            target_sitk = sitk.GetImageFromArray(target_arr)\n",
    "\n",
    "            img_sitk.SetSpacing(target_spacing)\n",
    "            target_sitk.SetSpacing(target_spacing)\n",
    "\n",
    "            img_save_path = os.path.join(images_save_root, pat+'_'+str(target_label)+'.nii.gz')\n",
    "            target_save_path = os.path.join(labels_save_root, pat+'_'+str(target_label)+'.nii.gz')\n",
    "\n",
    "        sitk.WriteImage(img_sitk, img_save_path)\n",
    "        sitk.WriteImage(target_sitk, target_save_path)\n",
    "\n",
    "# Save the GTV labels\n",
    "gtv_labels_save_path = os.path.join(save_root, 'gtv_labels.json')\n",
    "with open(gtv_labels_save_path, 'w') as f:\n",
    "    json.dump(total_gtv_labels, f, indent=4)\n",
    "print(\"\\nGTV labels successfully saved!\")\n",
    "print(f\"File path: {gtv_labels_save_path}\")\n",
    "print(f\"Total number of GTV instances: {len(total_gtv_labels)}\\n\")\n",
    "\n",
    "# Count the types of GTV labels\n",
    "gtv_type_counts = {gtv_type: 0 for gtv_type in gtv_types}\n",
    "for label in total_gtv_labels.values():\n",
    "    if label in gtv_type_counts:\n",
    "        gtv_type_counts[label] += 1\n",
    "\n",
    "print(\"GTV Subtype Distribution\")\n",
    "print(\"-\" * 30)\n",
    "for gtv_type, count in gtv_type_counts.items():\n",
    "    print(f\"{gtv_type:<5}: {count}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract APE (Anatomical Positional Embedding)\n",
    "\n",
    "To extract anatomical positional embeddings (APE) for each CT volume, please run the following notebook in a **separate Docker container** to avoid environment conflicts:\n",
    "\n",
    "**[extract_ape_NSCLC.ipynb](extract_ape_NSCLC.ipynb)**\n",
    "\n",
    "This will generate:\n",
    "- `.npy`: a single 3D tensor with **3 channels** in shape `(3, H, W, D)`  \n",
    "- `.nii.gz`: **three separate 3D NIfTI volumes** (one per channel)  \n",
    "\n",
    "The outputs will be saved to:\n",
    "\n",
    "```text\n",
    "/workspace/data/NSCLC/\n",
    "├── apes_npy/      # 3-channel APE tensors as .npy\n",
    "├── apes_nii/      # Individual APE channels as .nii.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Radiomics Features\n",
    "\n",
    "we extract handcrafted radiomics features from the preprocessed CT images and corresponding tumor masks using the [PyRadiomics](https://pyradiomics.readthedocs.io/) library.\n",
    "\n",
    "### Features extracted:\n",
    "For each tumor region, we compute four types of features:\n",
    "- 14 Shape-based features\n",
    "- 18 Histogram-based first-order features\n",
    "- 24 Texture features (GLCM)\n",
    "- 16 Texture features (GLSZM)\n",
    "\n",
    "### Output:\n",
    "All valid features are saved to a single CSV file:\n",
    "/workspace/data/NSCLC/radiomics_features.csv\n",
    "- Each row corresponds to one tumor sample (ID: `<patient_id>_<tumor_index>`)\n",
    "- Each column represents a radiomics feature\n",
    "- Samples with missing or invalid segmentations are excluded from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import radiomics\n",
    "from radiomics import featureextractor, firstorder, glcm, imageoperations, shape, glszm\n",
    "import json\n",
    "import warnings\n",
    "# Ignore specific warning messages\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from tqdm import tqdm\n",
    "logger = radiomics.logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(radiomics.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shape_Feature_Extract(ID, image, ROI):\n",
    "    ShapeFeatureExtractor = radiomics.shape.RadiomicsShape(image, ROI)\n",
    "    ShapeFeatureExtractor.enableAllFeatures()\n",
    "    ShapeFeatureExtractor.execute()\n",
    "    \n",
    "    result = pd.DataFrame([ShapeFeatureExtractor.featureValues])\n",
    "    result.insert(loc=0, column='ID', value=ID)\n",
    "    result.columns = ['ID']+['Shape_'+x for x in list(result.columns[1:])]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def Hist_Feature_Extract(ID, image, ROI):\n",
    "    settings = {'binCount': 128, 'interpolator' : None, 'verbose' : True}\n",
    "    \n",
    "    HistFeatureExtractor = radiomics.firstorder.RadiomicsFirstOrder(image, ROI, **settings)\n",
    "    HistFeatureExtractor.enableAllFeatures()\n",
    "    HistFeatureExtractor.execute()\n",
    "    \n",
    "    result = pd.DataFrame([HistFeatureExtractor.featureValues])\n",
    "    result.insert(loc=0, column='ID', value=ID)\n",
    "    result.columns = ['ID']+['Hist_'+x for x in list(result.columns[1:])]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def GLCM_Feature_Extract(ID, image, ROI):\n",
    "    settings = {'binCount': 128, 'interpolator' : None, 'verbose' : True}\n",
    "    \n",
    "    GLCMFeatureExtractor = radiomics.glcm.RadiomicsGLCM(image, ROI, **settings)\n",
    "    GLCMFeatureExtractor.enableAllFeatures()\n",
    "    GLCMFeatureExtractor.execute()\n",
    "    \n",
    "    result = pd.DataFrame([GLCMFeatureExtractor.featureValues])\n",
    "    result.insert(loc=0, column='ID', value=ID)\n",
    "    result.columns = ['ID']+['GLCM_'+x for x in list(result.columns[1:])]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def GLSZM_Feature_Extract(ID, image, ROI):\n",
    "    settings = {'binCount': 128, 'interpolator' : None, 'verbose' : True}\n",
    "    \n",
    "    GLSZMFeatureExtractor = radiomics.glszm.RadiomicsGLSZM(image, ROI, **settings)\n",
    "    GLSZMFeatureExtractor.enableAllFeatures()\n",
    "    GLSZMFeatureExtractor.execute()\n",
    "    \n",
    "    result = pd.DataFrame([GLSZMFeatureExtractor.featureValues])\n",
    "    result.insert(loc=0, column='ID', value=ID)\n",
    "    result.columns = ['ID']+['GLSZM_'+x for x in list(result.columns[1:])]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/NSCLC'\n",
    "imgs = os.listdir(os.path.join(data_root, 'labels'))\n",
    "\n",
    "shape_storage = dict()\n",
    "hist_storage = dict()\n",
    "glcm_storage = dict()\n",
    "glszm_storage = dict()\n",
    "\n",
    "Except_img = dict()\n",
    "\n",
    "for img in tqdm(imgs):\n",
    "    if 'nii' not in img:\n",
    "        continue\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(data_root, 'images', img)\n",
    "    seg_path = os.path.join(data_root, 'labels', img)\n",
    "\n",
    "    try:\n",
    "        img_sitk = sitk.ReadImage(img_path)\n",
    "        target_sitk = sitk.ReadImage(seg_path)\n",
    "        target_sitk = target_sitk != 0\n",
    "\n",
    "        # empty seg\n",
    "        if sitk.GetArrayFromImage(target_sitk).sum() == 0:\n",
    "            print(f'{img_name} has no segmentation')\n",
    "            Except_img[img_name] = 'no segmentation'\n",
    "            continue\n",
    "\n",
    "        shape_features = Shape_Feature_Extract(img_name, img_sitk, target_sitk)\n",
    "        hist_features = Hist_Feature_Extract(img_name, img_sitk, target_sitk)\n",
    "        glcm_features = GLCM_Feature_Extract(img_name, img_sitk, target_sitk)\n",
    "        glszm_features = GLSZM_Feature_Extract(img_name, img_sitk, target_sitk)\n",
    "    \n",
    "    except:\n",
    "        print(f'{img_name} feature extraction failed')\n",
    "        Except_img[img_name] = 'Feature extraction failed'\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        isnan = False\n",
    "\n",
    "        shape = dict(shape_features.iloc[0,1:])\n",
    "        for f in shape:\n",
    "            shape[f] = float(shape[f])\n",
    "            if np.isnan(shape[f]):\n",
    "                isnan = True\n",
    "        hist = dict(hist_features.iloc[0,1:])\n",
    "        for f in hist:\n",
    "            hist[f] = float(hist[f])  \n",
    "            if np.isnan(hist[f]):\n",
    "                isnan = True\n",
    "        glcm = dict(glcm_features.iloc[0,1:])\n",
    "        for f in glcm:\n",
    "            glcm[f] = float(glcm[f])\n",
    "            if np.isnan(glcm[f]):\n",
    "                isnan = True\n",
    "        glszm = dict(glszm_features.iloc[0,1:])\n",
    "        for f in glszm:\n",
    "            glszm[f] = float(glszm[f])\n",
    "            if np.isnan(glszm[f]):\n",
    "                isnan = True\n",
    "\n",
    "        if isnan:\n",
    "            print(f'{img_name} has nan value')\n",
    "            Except_img[img_name] = 'nan value'\n",
    "            continue\n",
    "\n",
    "        shape_storage[img_name] = shape\n",
    "        hist_storage[img_name] = hist\n",
    "        glcm_storage[img_name] = glcm\n",
    "        glszm_storage[img_name] = glszm\n",
    "\n",
    "        file_name = \"Exception_patients.json\"\n",
    "        file_path = os.path.join(data_root, file_name)\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(Except_img, f, indent=4)\n",
    "\n",
    "        # file_name = \"Shape_Features.json\"\n",
    "        # file_path = os.path.join(data_root, file_name)\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(shape_storage, f, indent=4)\n",
    "\n",
    "        # file_name = \"Hist_Features.json\"\n",
    "        # file_path = os.path.join(data_root, file_name)\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(hist_storage, f, indent=4)\n",
    "\n",
    "        # file_name = \"GLCM_Features.json\"\n",
    "        # file_path = os.path.join(data_root, file_name)\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(glcm_storage, f, indent=4)\n",
    "\n",
    "        # file_name = \"GLSZM_Features.json\"\n",
    "        # file_path = os.path.join(data_root, file_name)\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(glszm_storage, f, indent=4)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "file_name = \"Exception_patients.json\"\n",
    "file_path = os.path.join(data_root, file_name)\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(Except_img, f, indent=4)\n",
    "\n",
    "df_shape = pd.DataFrame(shape_storage).T\n",
    "df_hist = pd.DataFrame(hist_storage).T\n",
    "df_glcm = pd.DataFrame(glcm_storage).T\n",
    "df_glszm = pd.DataFrame(glszm_storage).T\n",
    "\n",
    "radiomics_features = pd.concat([df_shape, df_hist, df_glcm, df_glszm], axis=1)\n",
    "\n",
    "print('Shape features:', df_shape.shape)\n",
    "print('Histogram features:', df_hist.shape)\n",
    "print('GLCM features:', df_glcm.shape)\n",
    "print('GLSZM features:', df_glszm.shape)\n",
    "print('Total Radiomics features:', radiomics_features.shape)\n",
    "\n",
    "radiomics_features.index.name = 'ID'\n",
    "radiomics_features.reset_index(inplace=True)\n",
    "# radiomics_features.to_csv(os.path.join(data_root, 'radiomics_features.csv'), index=False)\n",
    "\n",
    "# print('\\nAll features saved to radiomics_features.csv')\n",
    "\n",
    "# Load the GTV labels\n",
    "gtv_labels_path = os.path.join(data_root, 'gtv_labels.json')\n",
    "with open(gtv_labels_path, 'r') as f:\n",
    "    gtv_labels = json.load(f)\n",
    "\n",
    "# Add GTV labels to the radiomics features DataFrame\n",
    "radiomics_features['GTV_label'] = radiomics_features['ID'].map(gtv_labels)\n",
    "\n",
    "# Save the updated DataFrame with GTV labels\n",
    "radiomics_features.to_csv(os.path.join(data_root, 'radiomics_features_with_gtv_labels.csv'), index=False)\n",
    "print('Radiomics features with GTV labels saved to radiomics_features_with_gtv_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert to HDF5 Format\n",
    "\n",
    "To facilitate fast I/O and unified data handling in downstream tasks, we convert all preprocessed data into a single HDF5 file.\n",
    "\n",
    "Each tumor sample includes:\n",
    "- **CT image** (`image`): 3D volume of shape (H, W, D)\n",
    "- **Tumor mask** (`tumor`): Binary segmentation mask aligned with the image\n",
    "- **Anatomical Positional Embedding** (`ape`): 3-channel tensor of shape (3, H, W, D)\n",
    "\n",
    "The output HDF5 file is structured as:\n",
    "\n",
    "```text\n",
    "/workspace/data/NSCLC/NSCLC_data.hdf5\n",
    "├── LUNG1-001_1/\n",
    "│   ├── image   → (H, W, D)\n",
    "│   ├── tumor   → (H, W, D)\n",
    "│   ├── ape     → (3, H, W, D)\n",
    "├── R01-001_1/\n",
    "│   ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/NSCLC'\n",
    "imgs = os.listdir(os.path.join(data_root, 'labels'))\n",
    "\n",
    "hdf5_save_path = os.path.join(data_root, 'NSCLC_data.hdf5')\n",
    "with h5py.File(hdf5_save_path, 'w') as hf:\n",
    "    pass\n",
    "\n",
    "for img in tqdm(imgs):\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(data_root, 'images', img)\n",
    "    seg_path = os.path.join(data_root, 'labels', img)\n",
    "    ape_path = os.path.join(data_root, 'apes_npy', img_name + '.npy')\n",
    "\n",
    "    img_arr = np.transpose(sitk.GetArrayFromImage(sitk.ReadImage(img_path)), (2, 1, 0))\n",
    "    seg_arr = np.transpose(sitk.GetArrayFromImage(sitk.ReadImage(seg_path)), (2, 1, 0))\n",
    "    ape_arr = np.load(ape_path)\n",
    "\n",
    "    if img_arr.shape != seg_arr.shape or img_arr.shape != ape_arr.shape[1:]:\n",
    "        print(f'{img_name} has inconsistent shapes: {img_arr.shape}, {seg_arr.shape}, {ape_arr.shape}')\n",
    "        continue\n",
    "\n",
    "    with h5py.File(hdf5_save_path, 'a') as hf:\n",
    "        grp = hf.create_group(img_name)\n",
    "        grp.create_dataset(\"image\", data=img_arr, compression=\"lzf\")\n",
    "        grp.create_dataset(\"tumor\", data=seg_arr, compression=\"lzf\")\n",
    "        grp.create_dataset(\"ape\", data=ape_arr, compression=\"lzf\")\n",
    "\n",
    "print(f'\\nAll data saved to {hdf5_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Radiomics JSONL Dataset\n",
    "\n",
    "In this step, we structure and export the radiomics feature dataset for model training or retrieval tasks.\n",
    "\n",
    "### Process Overview\n",
    "- Load `radiomics_features_with_gtv_labels.csv`, which contains extracted radiomics features and tumor subtype labels.\n",
    "- Use patient split information from `data_split.json` to assign each sample to `train`, `validation`, or `test` sets.\n",
    "\n",
    "### Radiomics Normalization\n",
    "- For each radiomics feature (excluding the label), compute the **min/max values from the training set only**.\n",
    "- Save the normalization statistics to: `/workspace/data/NSCLC/radiomics_features_min_max.json`\n",
    "  \n",
    "### Output Files\n",
    "The following .jsonl files are created in `/workspace/data/NSCLC/`, where each line is a JSON object in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"LUNG1-001_1\",\n",
    "  \"radiomics\": {\n",
    "    \"Shape_Elongation\": 0.676522,\n",
    "    \"Hist_Entropy\": 4.897085,\n",
    "    ...\n",
    "  },\n",
    "  \"label\": \"LCC\"\n",
    "}\n",
    "```\n",
    "\n",
    "- `train.jsonl`: : Training set samples\n",
    "- `val.jsonl`: Validation set samples\n",
    "- `test.jsonl`: Test set samples\n",
    "- `total.jsonl`: Full dataset (union of all above)\n",
    "\n",
    "### Label Distribution\n",
    "The script also prints out the number of samples per tumor subtype (`SCC`, `LCC`, `NOS`, `ADC`, `NaN`) in each split to help verify class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/NSCLC'\n",
    "data_split_path = os.path.join(data_root, 'data_split.json')\n",
    "with open(data_split_path, 'r') as f:\n",
    "    data_split = json.load(f)\n",
    "\n",
    "train_pats = data_split['train']\n",
    "val_pats = data_split['validation']\n",
    "test_pats = data_split['test']\n",
    "\n",
    "radiomics_features_path = os.path.join(data_root, 'radiomics_features_with_gtv_labels.csv')\n",
    "radiomics_features = pd.read_csv(radiomics_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "valset = []\n",
    "testset = []\n",
    "totalset = []\n",
    "\n",
    "train_radiomics = pd.DataFrame(None, columns=radiomics_features.columns[1:-1])\n",
    "val_radiomics = pd.DataFrame(None, columns=radiomics_features.columns[1:-1])\n",
    "test_radiomics = pd.DataFrame(None, columns=radiomics_features.columns[1:-1])\n",
    "\n",
    "for idx, row in tqdm(radiomics_features.iterrows(), total=len(radiomics_features)):\n",
    "    img_id = row['ID']\n",
    "    pat_id = \"_\".join(img_id.split('_')[:-1])\n",
    "\n",
    "    my_data = OrderedDict()\n",
    "    my_data['id'] = img_id\n",
    "    my_data['radiomics'] = row.iloc[1:-1].to_dict()\n",
    "    my_data['label'] = row['GTV_label']\n",
    "\n",
    "    if pat_id in train_pats:\n",
    "        trainset.append(my_data)\n",
    "        totalset.append(my_data)\n",
    "        train_radiomics.loc[len(train_radiomics)] = row.iloc[1:-1]\n",
    "    elif pat_id in val_pats:\n",
    "        valset.append(my_data)\n",
    "        totalset.append(my_data)\n",
    "        val_radiomics.loc[len(val_radiomics)] = row.iloc[1:-1]\n",
    "    elif pat_id in test_pats:\n",
    "        testset.append(my_data)\n",
    "        totalset.append(my_data)\n",
    "        test_radiomics.loc[len(test_radiomics)] = row.iloc[1:-1]\n",
    "    else:\n",
    "        print(f'Patient {pat_id} not found in any split, skipping...')\n",
    "\n",
    "print(f'\\nTotal number of patients: {len(radiomics_features)}')\n",
    "print(f'Train set size: {len(trainset)}')\n",
    "print(f'Validation set size: {len(valset)}')\n",
    "print(f'Test set size: {len(testset)}')\n",
    "\n",
    "radiomics_features_min_max = dict()\n",
    "for col in train_radiomics.columns:\n",
    "    radiomics_features_min_max[col] = (train_radiomics[col].min(), train_radiomics[col].max())\n",
    "\n",
    "radiomics_features_min_max_path = os.path.join(data_root, 'radiomics_features_min_max.json')\n",
    "with open(radiomics_features_min_max_path, 'w') as f:\n",
    "    json.dump(radiomics_features_min_max, f, indent=4)\n",
    "\n",
    "# print(train_radiomics.describe())\n",
    "\n",
    "train_scc = []\n",
    "train_lcc = []\n",
    "train_nos = []\n",
    "train_adc = []\n",
    "train_nan = []\n",
    "\n",
    "for data in trainset:\n",
    "    if data['label'] == 'SCC':\n",
    "        train_scc.append(data)\n",
    "    elif data['label'] == 'LCC':\n",
    "        train_lcc.append(data)\n",
    "    elif data['label'] == 'NOS':\n",
    "        train_nos.append(data)\n",
    "    elif data['label'] == 'ADC':\n",
    "        train_adc.append(data)\n",
    "    else:\n",
    "        train_nan.append(data)\n",
    "\n",
    "val_scc = []\n",
    "val_lcc = []\n",
    "val_nos = []\n",
    "val_adc = []\n",
    "val_nan = []\n",
    "\n",
    "for data in valset:\n",
    "    if data['label'] == 'SCC':\n",
    "        val_scc.append(data)\n",
    "    elif data['label'] == 'LCC':\n",
    "        val_lcc.append(data)\n",
    "    elif data['label'] == 'NOS':\n",
    "        val_nos.append(data)\n",
    "    elif data['label'] == 'ADC':\n",
    "        val_adc.append(data)\n",
    "    else:\n",
    "        val_nan.append(data)\n",
    "\n",
    "test_scc = []\n",
    "test_lcc = []\n",
    "test_nos = []\n",
    "test_adc = []\n",
    "test_nan = []\n",
    "\n",
    "for data in testset:\n",
    "    if data['label'] == 'SCC':\n",
    "        test_scc.append(data)\n",
    "    elif data['label'] == 'LCC':\n",
    "        test_lcc.append(data)\n",
    "    elif data['label'] == 'NOS':\n",
    "        test_nos.append(data)\n",
    "    elif data['label'] == 'ADC':\n",
    "        test_adc.append(data)\n",
    "    else:\n",
    "        test_nan.append(data)\n",
    "\n",
    "print(\"\\nGTV Subtype Distribution\")\n",
    "print(f'Train SCC: {len(train_scc)} / LCC: {len(train_lcc)} / NOS: {len(train_nos)} / ADC: {len(train_adc)} / NaN: {len(train_nan)}')\n",
    "print(f'Val SCC: {len(val_scc)} / LCC: {len(val_lcc)} / NOS: {len(val_nos)} / ADC: {len(val_adc)} / NaN: {len(val_nan)}')\n",
    "print(f'Test SCC: {len(test_scc)} / LCC: {len(test_lcc)} / NOS: {len(test_nos)} / ADC: {len(test_adc)} / NaN: {len(test_nan)}')\n",
    "\n",
    "# Save the datasets\n",
    "jsonl_file_path = os.path.join(data_root, 'train.jsonl')\n",
    "with open(jsonl_file_path, 'w') as f:\n",
    "    for data in trainset:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "jsonl_file_path = os.path.join(data_root, 'val.jsonl')\n",
    "with open(jsonl_file_path, 'w') as f:\n",
    "    for data in valset:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "jsonl_file_path = os.path.join(data_root, 'test.jsonl')\n",
    "with open(jsonl_file_path, 'w') as f:\n",
    "    for data in testset:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "jsonl_file_path = os.path.join(data_root, 'total.jsonl')\n",
    "with open(jsonl_file_path, 'w') as f:\n",
    "    for data in totalset:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
